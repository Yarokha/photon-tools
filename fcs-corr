#!/usr/bin/python

import argparse
from argparse import ArgumentParser
import warnings
import os.path

from photon_tools.correlate import corr, autocorr, corr_chunks, anomaly_likelihood
from photon_tools.io import read_photons

import numpy as np
import math

try:
    import matplotlib
    matplotlib.use('Agg')
    from matplotlib import pyplot as pl
    from matplotlib import cm
    mpl_available = True
except:
    mpl_available = False

verbose = False

description = """
fcs-corr computes cross- and auto-correlation functions on photon timestamp data.
In addition to the calculation of the correlation function itself, it supports
estimation of uncertainties, detection of anomalies (e.g. due to sample contamination),
and correction for instrument artifacts.
"""

parser = ArgumentParser(description=description,
                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
group = parser.add_argument_group('input options',
                                  'Options controlling input data')
group.add_argument('files', metavar='FILE', type=argparse.FileType('r'), nargs='+',
                   help='Input timestamp files')
group.add_argument('-j', '--jiffy', type=float, default=None,
                   help='Acquisition timebase period')
group.add_argument('-c', '--channels', type=str, metavar='CH,CH,{NAME}', action='append', default=[],
                   help='Channels between which to compute the correlation')
group.add_argument('-t', '--start-time', default=None, type=str,
                   help='Start time of usable data (seconds). Use + and - to indicate times relative to first and last timestamps of dataset, respectively.')
group.add_argument('-T', '--stop-time', default=None, type=str,
                   help='Stop time of usable data (seconds). Use + and - to indicate times relative to first and last timestamps of dataset, respectively.')
group.add_argument('-e', '--exclude', action='append', metavar='T-T',
                   help='Exclude a range of times (given in seconds)')

group = parser.add_argument_group('correlator options',
                                  'Options controlling how the correlation function is computed')
group.add_argument('-E', '--min-lag', type=float, default=5e-7,
                   help='Smallest lag to compute')
group.add_argument('-L', '--max-lag', type=float, default=1,
                   help='Longest lag to compute')
group.add_argument('-n', '--nchunks', type=int, default=10,
                   help='Number of subsamples to split data into for variance computation. If 1 the Wohland estimator is used.')
group.add_argument('-f', '--fineness', type=int, default=8,
                   help='Number of lags per octave')
group.add_argument('--anomaly-thresh', type=float,
                   help='Threshold on log-likelihood of chunk anomly model')

group = parser.add_argument_group('output options',
                                  'Options controlling how output is produced')
group.add_argument('-O', '--output', default='.',
                   help='Output directory')
group.add_argument('-v', '--verbose', action='store_true',
                   help='Produce debugging output')
group.add_argument('-p', '--plot', action='store_true',
                   help='Plot each computed correlation function')
group.add_argument('-l', '--label', type=str, default='corr',
                   help='Descriptive label for dataset shown in plot')
group.add_argument('-P', '--plot-type', default='png',
                   help='File format of plot output (svg, png, pdf, or ps)')

group = parser.add_argument_group('uncertainty estimation',
                                  'Options controlling how the uncertainty of the resulting correlation function is estimated')
group.add_argument('--chunks', action='store_true', help='Produce file containing chunks')
group.add_argument('--plot-chunks', action='store_true', help='Produce plot showing correlation functions of subsamples')
group.add_argument('--cross-chunks', action='store_true',
                   help='Use cross-correlations between non-cooccurent chunks to evaluate uncertainties')

group = parser.add_argument_group('afterpulsing correction',
                                  'Options controlling whether and how afterpulsing correction is applied')
group.add_argument('-A', '--afterpulse', default=None, type=argparse.FileType('r'),
                   metavar='FILE',
                   help='Uncorrelated timestamps for afterpulsing correction')
group.add_argument('--engine', default=None,
                   help='Which correlator engine to use (hphoton or favia)')
group.add_argument('--afterpulse-cutoff', default=None,
                   help="Don't apply afterpulsing correction beyond given lag")

args = parser.parse_args()
verbose = args.verbose

def parse_exclude(s):
    parts = s.split('-')
    if len(parts) != 2:
        raise RuntimeError('Expected range (e.g. "5-10" or "80-")')
    start, end = parts
    start = None if start == '' else float(start)
    end = None if end == '' else float(end)
    return (start, end)

exclude = [parse_exclude(e) for e in args.exclude] if args.exclude is not None else []

# Parse --channels
correlations = []
for s in args.channels:
    parts = s.split(',')
    try:
        a = int(parts[0])
        b = int(parts[1])
        if len(parts) == 3:
            name = parts[2]
        elif a == b:
            name = 'acorr-%d' % a
        else:
            name = 'xcorr-%d-%d' % (a,b)
        correlations.append((a,b,name))
    except Exception as e:
        raise RuntimeError('Error parsing --channels=%s: Expected pair of channels: %s' % (s, e))

# A reasonable default if --channels isn't specified
if len(correlations) == 0:
    a_channel = 0
    b_channel = 1
    correlations = [
        (a_channel, a_channel, 'acorr-%d' % a_channel),
        (b_channel, b_channel, 'acorr-%d' % b_channel),
        (a_channel, b_channel, 'xcorr-%d-%d' % (a_channel, b_channel))
    ]

def get_jiffy(timestamps):
    jiffy = timestamps.jiffy
    if args.jiffy is not None:
        jiffy = args.jiffy
    if jiffy is None:
        jiffy = 1. / 128e6
        print "Couldn't identify jiffy of file. Defaulting to %e s" % jiffy
    return jiffy

def afterpulsing_correction(ch):
    """
    Afterpulsing correction as described by,

      M. Zhao, L. Jin, B. Chen, et al.
      "Afterpulsing and its correction in fluorescence correlation spectroscopy
      experiments." _Applied Optics_ (2003)
    """
    f = read_photons.open(args.afterpulse.name)
    d = f.channel(ch)
    jiffy = get_jiffy(f)
    kwargs = {
        'jiffy': jiffy,
        'min_lag': args.min_lag,
        'max_lag': args.max_lag,
        'fineness': args.fineness,
        'engine': args.engine,
    }

    # TODO: Verify compatible jiffy
    uncorr_G = autocorr(d, **kwargs)
    mean_I = 1. * len(d) / (d[-1] - d[0])
    afterpulse_corr = (uncorr_G['G'] - 1) * mean_I / jiffy
    afterpulse_var = uncorr_G['var'] * (mean_I / jiffy)**2
    cutoff = args.afterpulse_cutoff
    if cutoff is not None:
        afterpulse_corr[uncorr_G['lag'] > cutoff] = 0
        afterpulse_var[uncorr_G['lag'] > cutoff] = 0
    return (afterpulse_corr, afterpulse_var)

if not os.path.isdir(args.output):
    os.makedirs(args.output)

# Memoize the correlation functions for afterpulsing correction
afterpulse_cache = {}

def in_range(bounds, arr):
    """
    :param bounds: a tuple of lower and upper bounds
    :param arr: an array of times
    :returns: a boolean array indicating which elements fall in the given time range.
    """
    lower, upper = bounds
    mask = True
    if lower is not None:
        mask = np.logical_and(mask, arr >= lower / jiffy)
    if upper is not None:
        mask = np.logical_and(mask, arr < upper / jiffy)
    return mask

for file in args.files:
    for (ac,bc,label) in correlations:
        fname = os.path.basename(file.name)
        output_base = os.path.join(args.output, fname+'.'+label)
        f = read_photons.open(file.name)

        jiffy = get_jiffy(f)
        ad, bd = f.channel(ac), f.channel(bc)
        na, nb = len(ad), len(bd)
        for bounds in exclude:
            ad = ad[in_range(bounds, ad)]
            bd = bd[in_range(bounds, bd)]

        if len(ad) == 0 or len(bd) == 0:
            print "No data in one channel, skipping"
            continue

        t0 = min(ad[0], bd[0])
        t1 = max(ad[-1], bd[-1])
        if args.start_time is not None:
            if args.start_time[0] == '-':
                start_time = t1 - float(args.start_time[1:]) / jiffy
            elif args.start_time[0] == '+':
                start_time = t0 + float(args.start_time[1:]) / jiffy
            else:
                start_time = float(args.start_time) / jiffy
            ad = ad[ad >= start_time]
            bd = bd[bd >= start_time]

        if args.stop_time is not None:
            if args.stop_time[0] == '-':
                stop_time = t1 - float(args.stop_time[1:]) / jiffy
            elif args.stop_time[0] == '+':
                stop_time = t0 + float(args.stop_time[1:]) / jiffy
            else:
                stop_time = float(args.stop_time) / jiffy
            ad = ad[ad < stop_time]
            bd = bd[bd < stop_time]

        print label

        print '  %20s, channel %d:' % (fname, ac)
        print '    %d events, %d after filtering' % (na, len(ad))
        durA = ad[-1] - ad[0]
        print '    duration = %d = %1.2f seconds' % (durA, (durA * jiffy))

        print '  %20s, channel %d:' % (fname, bc)
        print '    %d events, %d after filtering' % (nb, len(bd))
        durB = bd[-1] - bd[0]
        print '    duration = %d = %1.2f seconds' % (durB, (durB * jiffy))

        kwargs = {
            'jiffy': jiffy,
            'min_lag': args.min_lag,
            'max_lag': args.max_lag,
            'fineness': args.fineness,
            'verbose': args.verbose,
            'engine': args.engine,
        }

        c = None
        nchunks = args.nchunks

        if nchunks > 1:
            if durA / nchunks < 10 * args.max_lag \
               or durB / nchunks < 10 * args.max_lag:
                orig = nchunks
                nchunks = dur / nchunks / args.max_lag / 10
                warnings.warn('%s: Data set too short to compute %d chunks, will compute %d instead' % (fname, orig, nchunks))
            c,chunks = corr_chunks(ad, bd, n=nchunks, cross_chunks=args.cross_chunks,
                                   anomaly_thresh=args.anomaly_thresh, **kwargs)
            if args.chunks:
                np.savetxt(fname+'.chunks.'+label, np.vstack([c['lag'], chunks]).T)
        else:
            c = corr(ad, bd, **kwargs)

        if args.afterpulse is not None:
            if ac != bc:
                warnings.warn("Can only perform afterpulsing correction on autocorrelations, won't be performed")
            else:
                mu = 1.0 * na / durA
                if ac not in afterpulse_cache:
                    afterpulse_cache[ac] = afterpulsing_correction(ac)
                afterpulse_corr, afterpulse_var = afterpulse_cache[ac]
                c['G'] -= jiffy / mu * afterpulse_corr
                c['var'] += (jiffy / mu)**2 * afterpulse_var

        print '  G(%1.2e s) = %1.3f' % (c[0]['lag'], c[0]['G'])
        print '  G(%1.2e s) = %1.3f' % (c[-1]['lag'], c[-1]['G'])
        print

        with open(output_base, 'w') as f:
                f.write('# lag            G(tau)            variance\n')
                np.savetxt(f, c)

        if args.plot:
            if not mpl_available:
                raise RuntimeError('Matplotlib import failed, plotting not available')
            pl.clf()
            pl.xscale('log')
            pl.xlabel('Lag (s)')
            pl.ylabel('Correlation')
            pl.axhline(1, color='0.7')
            if nchunks > 1 and args.plot_chunks:
                for i in range(chunks.shape[0]):
                    pl.scatter(c['lag'], chunks[i,:], marker='+', alpha=0.6,
                               color='%f' % (0.3 + i * 0.7 / chunks.shape[0]))
                pl.scatter(c['lag'], np.mean(chunks, axis=0), marker='+', color='k')

            pl.errorbar(c['lag'], c['G'], yerr=np.sqrt(c['var']), fmt='+')
            pl.savefig(output_base+'.'+args.plot_type)

